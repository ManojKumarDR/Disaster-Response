{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: click==8.0.3 in c:\\users\\win\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 1)) (8.0.3)\n",
      "Requirement already satisfied: colorama==0.4.4 in c:\\users\\win\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 2)) (0.4.4)\n",
      "Requirement already satisfied: Flask==2.0.2 in c:\\users\\win\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 3)) (2.0.2)\n",
      "Requirement already satisfied: greenlet==1.1.2 in c:\\users\\win\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 4)) (1.1.2)\n",
      "Requirement already satisfied: itsdangerous==2.0.1 in c:\\users\\win\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 5)) (2.0.1)\n",
      "Requirement already satisfied: Jinja2==3.0.2 in c:\\users\\win\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 6)) (3.0.2)\n",
      "Requirement already satisfied: joblib==1.1.0 in c:\\users\\win\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 7)) (1.1.0)\n",
      "Requirement already satisfied: MarkupSafe==2.0.1 in c:\\users\\win\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 8)) (2.0.1)\n",
      "Requirement already satisfied: nltk==3.6.5 in c:\\users\\win\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 9)) (3.6.5)\n",
      "Requirement already satisfied: numpy==1.21.3 in c:\\users\\win\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 10)) (1.21.3)\n",
      "Requirement already satisfied: pandas==1.3.4 in c:\\users\\win\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 11)) (1.3.4)\n",
      "Requirement already satisfied: plotly==5.3.1 in c:\\users\\win\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 12)) (5.3.1)\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in c:\\users\\win\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 13)) (2.8.2)\n",
      "Requirement already satisfied: pytz==2021.3 in c:\\users\\win\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 14)) (2021.3)\n",
      "Requirement already satisfied: regex==2021.10.23 in c:\\users\\win\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 15)) (2021.10.23)\n",
      "Requirement already satisfied: scikit-learn==0.24.2 in c:\\users\\win\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 16)) (0.24.2)\n",
      "Requirement already satisfied: scipy==1.7.1 in c:\\users\\win\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 17)) (1.7.1)\n",
      "Requirement already satisfied: six==1.16.0 in c:\\users\\win\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 18)) (1.16.0)\n",
      "Requirement already satisfied: SQLAlchemy==1.4.26 in c:\\users\\win\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 19)) (1.4.26)\n",
      "Requirement already satisfied: tenacity==8.0.1 in c:\\users\\win\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 20)) (8.0.1)\n",
      "Requirement already satisfied: threadpoolctl==3.0.0 in c:\\users\\win\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 21)) (3.0.0)\n",
      "Requirement already satisfied: tqdm==4.62.3 in c:\\users\\win\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 22)) (4.62.3)\n",
      "Requirement already satisfied: Werkzeug==2.0.2 in c:\\users\\win\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 23)) (2.0.2)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\users\\win\\anaconda3\\lib\\site-packages (from click==8.0.3->-r requirements.txt (line 1)) (0.23)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\win\\anaconda3\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->click==8.0.3->-r requirements.txt (line 1)) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\win\\anaconda3\\lib\\site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->click==8.0.3->-r requirements.txt (line 1)) (7.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide the filepaths of the messages and categories datasets as the first and second argument respectively, as well as the filepath of the database to save the cleaned data to as the third argument. \n",
      "\n",
      "Example: python process_data.py disaster_messages.csv disaster_categories.csv DisasterResponse.db\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "def load_data(messages_filepath, categories_filepath):\n",
    "    \"\"\"\n",
    "    Function to :\n",
    "        - Loads data from csv files into dataframes and merge them afterwards\n",
    "    \n",
    "    Args:\n",
    "        messages_filepath (str): File path of messages\n",
    "        categories_filepath (str): File pathe of categories\n",
    "    \n",
    "    Returns:\n",
    "        pandas dataframe: Merged dataframe containing messages and categories\n",
    "    \"\"\"\n",
    "    messages = pd.read_csv(messages_filepath)\n",
    "    categories = pd.read_csv(categories_filepath)\n",
    "        \n",
    "    return pd.merge(messages, categories, how = 'inner', on = ['id'])\n",
    "\n",
    "\n",
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Function to :\n",
    "        - Clean the data loaded as merged pandas dataframe\n",
    "    \n",
    "    Args:\n",
    "        df (pandas dataframe): Dataframe containing messages and categories\n",
    "    \n",
    "    Returns:\n",
    "        pandas dataframe: Cleaned dataframe\n",
    "    \"\"\"\n",
    "    # create a dataframe of the 36 individual category columns\n",
    "    categories = df['categories'].str.split( \";\", expand = True )\n",
    "    \n",
    "    # extract a list of new column names for categories.\n",
    "    category_colnames = categories.iloc[0].apply( lambda x: x[:-2] ).tolist()\n",
    "    \n",
    "    # rename the columns of `categories`\n",
    "    categories.columns = category_colnames\n",
    "    \n",
    "    for column in categories:\n",
    "    \n",
    "        # set each value to be the last character of the string\n",
    "        categories[column] = categories[column].str[-1]\n",
    "        \n",
    "        # convert column from string to numeric\n",
    "        categories[column] = pd.to_numeric(categories[column])\n",
    "    \n",
    "    # drop the original categories column from `df`\n",
    "    df = df.drop( columns = ['categories'], axis = 1 )\n",
    "    \n",
    "    # concatenate the original dataframe with the new `categories` dataframe\n",
    "    df = pd.concat( [df, categories], axis = 1 )\n",
    "    \n",
    "    # drop duplicates\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def save_data(df, database_filename):\n",
    "    \"\"\"\n",
    "    Function to :\n",
    "        - Save cleaned data as sqlite database\n",
    "    \n",
    "    Args:\n",
    "        df (pandas dataframe): Dataframe containing messages and categories\n",
    "        database_filename (str): Database name\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    engine = create_engine('sqlite:///' + database_filename)\n",
    "    \n",
    "    database_name = database_filename.replace(\".db\",\"\")\n",
    "    df.to_sql(database_name, engine, index = False)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    if len(sys.argv) == 4:\n",
    "\n",
    "        messages_filepath, categories_filepath, database_filepath = sys.argv[1:]\n",
    "\n",
    "        print('Loading data...\\n    MESSAGES: {}\\n    CATEGORIES: {}'\n",
    "              .format(messages_filepath, categories_filepath))\n",
    "        df = load_data(messages_filepath, categories_filepath)\n",
    "\n",
    "        print('Cleaning data...')\n",
    "        df = clean_data(df)\n",
    "        \n",
    "        print('Saving data...\\n    DATABASE: {}'.format(database_filepath))\n",
    "        save_data(df, database_filepath)\n",
    "        \n",
    "        print('Cleaned data saved to database!')\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        print('Please provide the filepaths of the messages and categories '\\\n",
    "              'datasets as the first and second argument respectively, as '\\\n",
    "              'well as the filepath of the database to save the cleaned data '\\\n",
    "              'to as the third argument. \\n\\nExample: python process_data.py '\\\n",
    "              'disaster_messages.csv disaster_categories.csv '\\\n",
    "              'DisasterResponse.db')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data\\\\disaster_categories.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-0;medi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-1;medi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-0;medi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>related-1;request-1;offer-0;aid_related-1;medi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-0;medi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                         categories\n",
       "0   2  related-1;request-0;offer-0;aid_related-0;medi...\n",
       "1   7  related-1;request-0;offer-0;aid_related-1;medi...\n",
       "2   8  related-1;request-0;offer-0;aid_related-0;medi...\n",
       "3   9  related-1;request-1;offer-0;aid_related-1;medi...\n",
       "4  12  related-1;request-0;offer-0;aid_related-0;medi..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Weather update - a cold front from Cuba that c...</td>\n",
       "      <td>Un front froid se retrouve sur Cuba ce matin. ...</td>\n",
       "      <td>direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>Is the Hurricane over or is it not over</td>\n",
       "      <td>Cyclone nan fini osinon li pa fini</td>\n",
       "      <td>direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>Looking for someone but no name</td>\n",
       "      <td>Patnm, di Maryani relem pou li banm nouvel li ...</td>\n",
       "      <td>direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>says: west side of Haiti, rest of the country ...</td>\n",
       "      <td>facade ouest d Haiti et le reste du pays aujou...</td>\n",
       "      <td>direct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            message  \\\n",
       "0   2  Weather update - a cold front from Cuba that c...   \n",
       "1   7            Is the Hurricane over or is it not over   \n",
       "2   8                    Looking for someone but no name   \n",
       "3   9  UN reports Leogane 80-90 destroyed. Only Hospi...   \n",
       "4  12  says: west side of Haiti, rest of the country ...   \n",
       "\n",
       "                                            original   genre  \n",
       "0  Un front froid se retrouve sur Cuba ce matin. ...  direct  \n",
       "1                 Cyclone nan fini osinon li pa fini  direct  \n",
       "2  Patnm, di Maryani relem pou li banm nouvel li ...  direct  \n",
       "3  UN reports Leogane 80-90 destroyed. Only Hospi...  direct  \n",
       "4  facade ouest d Haiti et le reste du pays aujou...  direct  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data\\\\disaster_messages.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sqlite3 as sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = \"DisasterResponse.db\"\n",
    "connection = sql.connect(database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''SELECT * FROM DisasterResponse'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql 'SELECT * FROM DisasterResponse': no such table: DisasterResponse",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2055\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2056\u001b[1;33m             \u001b[0mcur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2057\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mcur\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOperationalError\u001b[0m: no such table: DisasterResponse",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-0cc4eec7562e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_sql_query\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mread_sql_query\u001b[1;34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize, dtype)\u001b[0m\n\u001b[0;32m    441\u001b[0m         \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m         \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m         \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m     )\n\u001b[0;32m    445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mread_query\u001b[1;34m(self, sql, index_col, coerce_float, params, parse_dates, chunksize, dtype)\u001b[0m\n\u001b[0;32m   2114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2115\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_convert_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2116\u001b[1;33m         \u001b[0mcursor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2117\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcol_desc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol_desc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2066\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2067\u001b[0m             \u001b[0mex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDatabaseError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Execution failed on sql '{args[0]}': {exc}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2068\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mex\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2069\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2070\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDatabaseError\u001b[0m: Execution failed on sql 'SELECT * FROM DisasterResponse': no such table: DisasterResponse"
     ]
    }
   ],
   "source": [
    "df = pd.read_sql_query(query,connection)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<----- Loading data... ----->\n",
      "    DATABASE: -f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Win\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Win\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Win\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Win\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-91011da542b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-50-91011da542b9>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n<----- Loading data... ----->\\n    DATABASE: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatabase_filepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatabase_filepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    281\u001b[0m         X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\\\n\u001b[0;32m    282\u001b[0m                                                             test_size = 0.2)\n",
      "\u001b[1;32m<ipython-input-50-91011da542b9>\u001b[0m in \u001b[0;36mload_data\u001b[1;34m(database_filepath)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mindexes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatabase_filepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0mdatabase_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatabase_filepath\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindexes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_sql_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatabase_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download(['punkt', 'stopwords', 'wordnet', 'averaged_perceptron_tagger'])\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import make_scorer, f1_score,\\\n",
    "                            precision_recall_fscore_support,\\\n",
    "                            classification_report\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "def load_data(database_filepath):\n",
    "    \"\"\"\n",
    "    Function to :\n",
    "        - Load data from database\n",
    "    \n",
    "    Args:\n",
    "        database_filepath (str): File path of database\n",
    "    \n",
    "    Returns:\n",
    "        pandas dataframe: Merged dataframe containing messages and categories\n",
    "    \"\"\"\n",
    "    engine = create_engine('sqlite:///' + database_filepath)\n",
    "    \n",
    "    indexes = [x.start() for x in re.finditer('/', database_filepath)]\n",
    "    database_name = database_filepath[indexes[-1] + 1: -3]\n",
    "    \n",
    "    df = pd.read_sql_table(database_name, engine)\n",
    "    X = df[\"message\"]\n",
    "    Y = df.iloc[:,4:]\n",
    "    category_names = Y.columns.tolist()\n",
    "    \n",
    "    return X, Y, category_names\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Function to :\n",
    "        - process text data\n",
    "    \n",
    "    Args:\n",
    "        text (str): string of messages\n",
    "    \n",
    "    Returns:\n",
    "        clean_tokens (list): list of tokenized text data\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation characters; prefixed with r to indicate that \n",
    "    # it is a regular expression\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    \n",
    "    # tokenize text / Split text into words using NLTK\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    tokens = [w for w in tokens if w not in stopwords.words(\"english\")]\n",
    "    \n",
    "    # Reduce words to their root form and remove white space\n",
    "    clean_tokens = [WordNetLemmatizer().lemmatize(w).strip() for w in tokens]\n",
    "            \n",
    "    return clean_tokens\n",
    "\n",
    "\n",
    "class StartingVerbExtractor(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    \"\"\" \n",
    "    Class for:\n",
    "        -extracting whether each sentence started with a verb,\n",
    "         creating a new feature\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    def starting_verb(self, text):\n",
    "\n",
    "        sentence_list = nltk.sent_tokenize(text) # tokenize by sentences\n",
    "\n",
    "        for sentence in sentence_list:\n",
    "            \n",
    "            # tokenize each sentence into words and tag part of speech\n",
    "            pos_tags = nltk.pos_tag(tokenize(sentence))\n",
    "            \n",
    "            # Check if pos_tags is empty; true if pos_tags is not empty\n",
    "            if pos_tags:\n",
    "                \n",
    "                # index pos_tags to get the first word and part of speech tag\n",
    "                first_word, first_tag = pos_tags[0]\n",
    "                \n",
    "                # return true if the first word is an appropriate verb \n",
    "                # or RT for retweet\n",
    "                if first_tag in ['VB', 'VBP'] or first_word == 'RT':\n",
    "                    return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def fit(self, x, y = None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \n",
    "        # apply starting_verb function to all values in X\n",
    "        X_tagged = pd.Series(X).apply(self.starting_verb)\n",
    "        \n",
    "        return pd.DataFrame(X_tagged)\n",
    "\n",
    "\n",
    "def multi_output_fscore(Y_true, Y_pred):\n",
    "    \"\"\"\n",
    "    Function to :\n",
    "        - be the input of function make_scorer(), thus being the scoring method \n",
    "          of the grid search object created by GridSearchCV()\n",
    "    \n",
    "    Args:\n",
    "        Y_true (pandas dataframe): labels\n",
    "        Y_pred (pandas dataframe): predictions\n",
    "        average (str): this determines the type of averaging performed on the \n",
    "                       data\n",
    "    \n",
    "    Returns:\n",
    "        fscore_list.mean() (float): mean of f1-score\n",
    "    \"\"\"\n",
    "    fscore_list = []\n",
    "    \n",
    "    for i in range(0, Y_true.shape[1]):\n",
    "\n",
    "        f_score = f1_score(y_true = Y_true.iloc[:, i],\\\n",
    "                           y_pred = Y_pred[:, i],\\\n",
    "                           average = 'weighted',\\\n",
    "                           zero_division = 0)\n",
    "                            \n",
    "        fscore_list.append(f_score)\n",
    "\n",
    "    fscore_list = np.array(fscore_list)\n",
    "\n",
    "    return fscore_list.mean()\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    \"\"\"\n",
    "    Function to :\n",
    "        - Build the model with the parameters selected by grid search\n",
    "    \n",
    "    Args:\n",
    "        None\n",
    "    \n",
    "    Returns:\n",
    "        cv (estimator): machine learning model trained with the parameters \n",
    "                        selected by grid search\n",
    "    \"\"\"    \n",
    "    # build pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "    \n",
    "            ('text_pipeline', Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer = tokenize)),\n",
    "                ('tfidf', TfidfTransformer())\n",
    "            ])),\n",
    "    \n",
    "            ('starting_verb', StartingVerbExtractor())\n",
    "        ])),\n",
    "    \n",
    "        ('clf', MultiOutputClassifier(AdaBoostClassifier(random_state = 42)))\n",
    "    ])\n",
    "\n",
    "    # specify parameters for grid search\n",
    "    parameters = {\n",
    "                    'features__text_pipeline__vect__ngram_range': [(1, 2)],\\\n",
    "                    'features__text_pipeline__vect__max_features': [5000],\\\n",
    "                    'clf__estimator__algorithm': ['SAMME.R'],\\\n",
    "                    'clf__estimator__base_estimator': [None],\\\n",
    "                    'clf__estimator__learning_rate': [1],\\\n",
    "                    'clf__estimator__n_estimators': [50, 100]\n",
    "    }\n",
    "    \n",
    "    scorer = make_scorer(multi_output_fscore, greater_is_better = True)\n",
    "\n",
    "    # create grid search object\n",
    "    cv = GridSearchCV(estimator = pipeline,\\\n",
    "                      param_grid = parameters,\\\n",
    "                      scoring = scorer,\\\n",
    "                      n_jobs = 1,\\\n",
    "                      refit = True,\\\n",
    "                      cv = 2,\\\n",
    "                      verbose = 4,\\\n",
    "                      error_score = 'raise')\n",
    "    \n",
    "    return cv\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test, Y_test, category_names, average = 'weighted'):\n",
    "    \"\"\"\n",
    "    Function to :\n",
    "        - Evaluate model\n",
    "    \n",
    "    Args:\n",
    "        model (estimator): machine learning model\n",
    "        X_test (pandas series): test data set of X\n",
    "        Y_test (pandas dataframe): test data set of Y\n",
    "        category_names (list): name of categories\n",
    "        average (str): this determines the type of averaging performed \n",
    "                       on the data\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Predict test data\n",
    "    Y_pred = model.predict(X_test)\n",
    "    \n",
    "    results = pd.DataFrame(columns = ['Category', 'Precision', 'Recall',\\\n",
    "                                      'F-score'])\n",
    "\n",
    "    for i in range(len(category_names)):\n",
    "\n",
    "        category = category_names[i]\n",
    "        \n",
    "        precision, recall, f_score, support =\\\n",
    "        precision_recall_fscore_support(Y_test[category],\\\n",
    "                                        Y_pred[:, i],\\\n",
    "                                        average = average,\\\n",
    "                                        zero_division = 0 \n",
    "        )\n",
    "        \n",
    "        results = results.append({'Category': category,\\\n",
    "                                  'Precision': precision,\\\n",
    "                                  'Recall': recall,\\\n",
    "                                  'F-score': f_score},\\\n",
    "                                  ignore_index = True)\n",
    "\n",
    "    print('Mean Precision:', results['Precision'].mean())\n",
    "    print('Mean Recall:', results['Recall'].mean())\n",
    "    print('Mean F_score:', results['F-score'].mean())\n",
    "    print('\\n--------------------Classification Report--------------------\\n')\n",
    "    \n",
    "    for i in range(len(category_names)):\n",
    "\n",
    "        category = category_names[i]\n",
    "        print(category)\n",
    "        print(classification_report(Y_test[category],\\\n",
    "                                    Y_pred[:, i],\\\n",
    "                                    zero_division = 0))\n",
    "\n",
    "\n",
    "def save_model(model, model_filepath):\n",
    "    \"\"\"\n",
    "    Function to :\n",
    "        - Saves model as pickle file\n",
    "    \n",
    "    Args:\n",
    "        model (estimator): machine learning model\n",
    "        model_filepath (str): path where model will be saved\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with open(model_filepath, 'wb') as file:  \n",
    "        pickle.dump(model, file)\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    if len(sys.argv) == 3:\n",
    "        \n",
    "        database_filepath, model_filepath = sys.argv[1:]\n",
    "        \n",
    "        print('\\n<----- Loading data... ----->\\n    DATABASE: {}'.format(database_filepath))\n",
    "        X, Y, category_names = load_data(database_filepath)\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\\\n",
    "                                                            test_size = 0.2)\n",
    "        \n",
    "        print('\\n<----- Building model... ----->')\n",
    "        model = build_model()\n",
    "        \n",
    "        print('\\n<----- Training model... ----->')\n",
    "        model.fit(X_train, Y_train)\n",
    "        print('\\nBest parameters found by grid search:\\n{}'.format(model.best_params_))\n",
    "        \n",
    "        print('\\n<----- Evaluating model... ----->')\n",
    "        evaluate_model(model, X_test, Y_test, category_names)\n",
    "\n",
    "        print('\\n<----- Saving model... ----->\\n    MODEL: {}'.format(model_filepath))\n",
    "        save_model(model, model_filepath)\n",
    "\n",
    "        print('\\n\\nTrained model saved!')\n",
    "\n",
    "    else:\n",
    "        print('Please provide the filepath of the disaster messages database '\\\n",
    "              'as the first argument and the filepath of the pickle file to '\\\n",
    "              'save the model to as the second argument. \\n\\nExample: python '\\\n",
    "              'train_classifier.py ../data/DisasterResponse.db classifier.pkl')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-8debf133f600>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msqlalchemy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcreate_engine\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplotting\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mreturn_figure\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcustom_scorer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmulti_output_fscore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcustom_transformer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mStartingVerbExtractor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import plotly\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from flask import Flask\n",
    "from flask import render_template, request, jsonify\n",
    "from plotly.graph_objs import Bar\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from utils.plotting import return_figure\n",
    "from utils.custom_scorer import multi_output_fscore\n",
    "from utils.custom_transformer import tokenize, StartingVerbExtractor\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# load data\n",
    "database_name = 'DisasterResponse'\n",
    "engine = create_engine('sqlite:///../data/{}.db'.format(database_name))\n",
    "df = pd.read_sql_table('{}'.format(database_name), engine)\n",
    "\n",
    "# load model\n",
    "model_name = 'classifier'\n",
    "model = joblib.load(\"../models/{}.pkl\".format(model_name))\n",
    "\n",
    "\n",
    "# index webpage displays cool visuals and receives user input text for model\n",
    "@app.route('/')\n",
    "@app.route('/index')\n",
    "def index():\n",
    "    \n",
    "    # create visuals\n",
    "    graphs = return_figure(df = df)\n",
    "    \n",
    "    # encode plotly graphs in JSON\n",
    "    ids = [\"graph-{}\".format(i) for i, _ in enumerate(graphs)]\n",
    "    graphJSON = json.dumps(graphs, cls = plotly.utils.PlotlyJSONEncoder)\n",
    "    \n",
    "    # render web page with plotly graphs\n",
    "    return render_template('master.html', ids = ids, graphJSON = graphJSON)\n",
    "\n",
    "\n",
    "# web page that handles user query and displays model results\n",
    "@app.route('/go')\n",
    "def go():\n",
    "    # save user input in query\n",
    "    query = request.args.get('query', '') \n",
    "\n",
    "    # use model to predict classification for query\n",
    "    classification_labels = model.predict([query])[0]\n",
    "    classification_results = dict(zip(df.columns[4:], classification_labels))\n",
    "\n",
    "    # This will render the go.html Please see that file. \n",
    "    return render_template(\n",
    "        'go.html',\n",
    "        query=query,\n",
    "        classification_result=classification_results\n",
    "    )\n",
    "\n",
    "\n",
    "def main():\n",
    "    app.run(host = '0.0.0.0', port = 3001, debug = True)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipython-sql in c:\\users\\win\\anaconda3\\lib\\site-packages (0.4.1)\n",
      "Requirement already satisfied: ipython-genutils>=0.1.0 in c:\\users\\win\\anaconda3\\lib\\site-packages (from ipython-sql) (0.2.0)\n",
      "Requirement already satisfied: sqlparse in c:\\users\\win\\anaconda3\\lib\\site-packages (from ipython-sql) (0.4.2)\n",
      "Requirement already satisfied: ipython>=1.0 in c:\\users\\win\\anaconda3\\lib\\site-packages (from ipython-sql) (7.8.0)\n",
      "Requirement already satisfied: six in c:\\users\\win\\anaconda3\\lib\\site-packages (from ipython-sql) (1.16.0)\n",
      "Requirement already satisfied: prettytable<1 in c:\\users\\win\\anaconda3\\lib\\site-packages (from ipython-sql) (0.7.2)\n",
      "Requirement already satisfied: sqlalchemy>=0.6.7 in c:\\users\\win\\anaconda3\\lib\\site-packages (from ipython-sql) (1.4.26)\n",
      "Requirement already satisfied: decorator in c:\\users\\win\\anaconda3\\lib\\site-packages (from ipython>=1.0->ipython-sql) (4.4.0)\n",
      "Requirement already satisfied: colorama; sys_platform == \"win32\" in c:\\users\\win\\anaconda3\\lib\\site-packages (from ipython>=1.0->ipython-sql) (0.4.4)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in c:\\users\\win\\anaconda3\\lib\\site-packages (from ipython>=1.0->ipython-sql) (2.0.10)\n",
      "Requirement already satisfied: jedi>=0.10 in c:\\users\\win\\anaconda3\\lib\\site-packages (from ipython>=1.0->ipython-sql) (0.15.1)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\win\\anaconda3\\lib\\site-packages (from ipython>=1.0->ipython-sql) (0.7.5)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\win\\anaconda3\\lib\\site-packages (from ipython>=1.0->ipython-sql) (41.4.0)\n",
      "Requirement already satisfied: backcall in c:\\users\\win\\anaconda3\\lib\\site-packages (from ipython>=1.0->ipython-sql) (0.1.0)\n",
      "Requirement already satisfied: pygments in c:\\users\\win\\anaconda3\\lib\\site-packages (from ipython>=1.0->ipython-sql) (2.4.2)\n",
      "Requirement already satisfied: traitlets>=4.2 in c:\\users\\win\\anaconda3\\lib\\site-packages (from ipython>=1.0->ipython-sql) (4.3.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17; python_version >= \"3\" and (platform_machine == \"aarch64\" or (platform_machine == \"ppc64le\" or (platform_machine == \"x86_64\" or (platform_machine == \"amd64\" or (platform_machine == \"AMD64\" or (platform_machine == \"win32\" or platform_machine == \"WIN32\")))))) in c:\\users\\win\\anaconda3\\lib\\site-packages (from sqlalchemy>=0.6.7->ipython-sql) (1.1.2)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\users\\win\\anaconda3\\lib\\site-packages (from sqlalchemy>=0.6.7->ipython-sql) (0.23)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\win\\anaconda3\\lib\\site-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=1.0->ipython-sql) (0.1.7)\n",
      "Requirement already satisfied: parso>=0.5.0 in c:\\users\\win\\anaconda3\\lib\\site-packages (from jedi>=0.10->ipython>=1.0->ipython-sql) (0.5.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\win\\anaconda3\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->sqlalchemy>=0.6.7->ipython-sql) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\win\\anaconda3\\lib\\site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->sqlalchemy>=0.6.7->ipython-sql) (7.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install ipython-sql"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
